{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xSU5slg3nXy",
        "outputId": "5cb4e4bb-92e7-41d7-e501-9a77b8d56dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MuhammadAmmarSaleem/Urdu-Summary-Dataset.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo3JxnUt3rdw",
        "outputId": "b1a4662f-cc2c-40e7-f271-f44636aa6f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Urdu-Summary-Dataset'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 121 (delta 6), reused 121 (delta 6), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (121/121), 153.52 KiB | 5.48 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voUaomHnRxvW",
        "outputId": "cfb2a65e-37b3-432a-8636-f9cf95fb9f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install urduhack[tf]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HlQ8oxgCxr1",
        "outputId": "25480cc4-d430-40b0-d372-8e40585feac7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting urduhack[tf]\n",
            "  Downloading urduhack-1.1.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-datasets~=3.1\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from urduhack[tf]) (2022.10.31)\n",
            "Collecting tf2crf\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting Click~=7.1\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.10/dist-packages (from urduhack[tf]) (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (2.12.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.16.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (3.20.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (2.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (16.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (67.7.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (3.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.54.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (0.32.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (23.3.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (3.3.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (1.22.4)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (2.12.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (23.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (0.4.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (4.5.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.2->urduhack[tf]) (0.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (2.27.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (4.65.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (0.18.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (2.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets~=3.1->urduhack[tf]) (23.1.0)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons>=0.8.2\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.2->urduhack[tf]) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->urduhack[tf]) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow~=2.2->urduhack[tf]) (1.10.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets~=3.1->urduhack[tf]) (2022.12.7)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (0.7.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (2.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (2.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (3.4.3)\n",
            "Collecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets~=3.1->urduhack[tf]) (1.59.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (0.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow~=2.2->urduhack[tf]) (3.2.2)\n",
            "Installing collected packages: typeguard, dill, Click, tensorflow-addons, tensorflow-datasets, tf2crf, urduhack\n",
            "  Attempting uninstall: Click\n",
            "    Found existing installation: click 8.1.3\n",
            "    Uninstalling click-8.1.3:\n",
            "      Successfully uninstalled click-8.1.3\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.9.2\n",
            "    Uninstalling tensorflow-datasets-4.9.2:\n",
            "      Successfully uninstalled tensorflow-datasets-4.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 2.2.4 requires click>=8.0, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Click-7.1.2 dill-0.3.6 tensorflow-addons-0.20.0 tensorflow-datasets-3.2.1 tf2crf-0.1.33 typeguard-2.13.3 urduhack-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urduhack\n",
        "\n",
        "# Downloading models\n",
        "urduhack.download()\n",
        "from urduhack.tokenization import sentence_tokenizer\n",
        "from urduhack.tokenization import word_tokenizer\n",
        "from urduhack import normalize\n",
        "from urduhack.preprocessing import normalize_whitespace, remove_punctuation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWD92lxeDHi3",
        "outputId": "5e0f70b5-e7cd-4af1-db59-7a8bd7e4f55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/urduhack/resources/releases/download/word_tokenizer/word_tokenizer.zip\n",
            "36788015/36788015 [==============================] - 2s 0us/step\n",
            "Downloading data from https://github.com/urduhack/resources/releases/download/pos_tagger/pos_tagger.zip\n",
            "2761433/2761433 [==============================] - 1s 0us/step\n",
            "Downloading data from https://github.com/urduhack/resources/releases/download/ner/ner.zip\n",
            "11723346/11723346 [==============================] - 1s 0us/step\n",
            "Downloading data from https://github.com/urduhack/resources/releases/download/lemmatizer/ur_lemma_lookup.zip\n",
            "89078/89078 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words from text\n",
        "from typing import FrozenSet\n",
        "\n",
        "# Urdu Language Stop words list\n",
        "STOP_WORDS: FrozenSet[str] = frozenset(\"\"\"\n",
        " آ آئی آئیں آئے آتا آتی آتے آس آمدید آنا آنسہ آنی آنے آپ آگے آہ آہا آیا اب ابھی ابے\n",
        " ارے اس اسکا اسکی اسکے اسی اسے اف افوہ البتہ الف ان اندر انکا انکی انکے انہوں انہی انہیں اوئے اور اوپر\n",
        " اوہو اپ اپنا اپنوں اپنی اپنے اپنےآپ اکثر اگر اگرچہ اہاہا ایسا ایسی ایسے ایک بائیں بار بارے بالکل باوجود باہر\n",
        " بج بجے بخیر بشرطیکہ بعد بعض بغیر بلکہ بن بنا بناؤ بند بڑی بھر بھریں بھی بہت بہتر تاکہ تاہم تب تجھ\n",
        " تجھی تجھے ترا تری تلک تم تمام تمہارا تمہاروں تمہاری تمہارے تمہیں تو تک تھا تھی تھیں تھے تیرا تیری تیرے\n",
        " جا جاؤ جائیں جائے جاتا جاتی جاتے جانی جانے جب جبکہ جدھر جس جسے جن جناب جنہوں جنہیں جو جہاں جی جیسا\n",
        " جیسوں جیسی جیسے حالانکہ حالاں حصہ حضرت خاطر خالی خواہ خوب خود دائیں درمیان دریں دو دوران دوسرا دوسروں دوسری دوں\n",
        " دکھائیں دی دیئے دیا دیتا دیتی دیتے دیر دینا دینی دینے دیکھو دیں دیے دے ذریعے رکھا رکھتا رکھتی رکھتے رکھنا رکھنی\n",
        " رکھنے رکھو رکھی رکھے رہ رہا رہتا رہتی رہتے رہنا رہنی رہنے رہو رہی رہیں رہے ساتھ سامنے ساڑھے سب سبھی\n",
        " سراسر سمیت سوا سوائے سکا سکتا سکتے سہ سہی سی سے شاید شکریہ صاحب صاحبہ صرف ضرور طرح طرف طور علاوہ عین\n",
        " فقط فلاں فی قبل قطا لئے لائی لائے لاتا لاتی لاتے لانا لانی لانے لایا لو لوجی لوگوں لگ لگا لگتا\n",
        " لگتی لگی لگیں لگے لہذا لی لیا لیتا لیتی لیتے لیکن لیں لیے لے ماسوا مت مجھ مجھی مجھے محترم محترمہ محض\n",
        " مرا مرحبا مری مرے مزید مس مسز مسٹر مطابق مل مکرمی مگر مگھر مہربانی میرا میروں میری میرے میں نا نزدیک\n",
        " نما نہ نہیں نیز نیچے نے و وار واسطے واقعی والا والوں والی والے واہ وجہ ورنہ وغیرہ ولے وگرنہ وہ وہاں\n",
        " وہی وہیں ویسا ویسے ویں پاس پایا پر پس پلیز پون پونی پونے پھر پہ پہلا پہلی پہلے پیر پیچھے چاہئے\n",
        " چاہتے چاہیئے چاہے چلا چلو چلیں چلے چناچہ چند چونکہ چکی چکیں چکے ڈالنا ڈالنی ڈالنے ڈالے کئے کا کاش کب کبھی\n",
        " کدھر کر کرتا کرتی کرتے کرم کرنا کرنے کرو کریں کرے کس کسی کسے کم کن کنہیں کو کوئی کون کونسا\n",
        " کونسے کچھ کہ کہا کہاں کہہ کہی کہیں کہے کی کیا کیسا کیسے کیونکر کیونکہ کیوں کیے کے گئی گئے گا گنا\n",
        " گو گویا گی گیا ہائیں ہائے ہاں ہر ہرچند ہرگز ہم ہمارا ہماری ہمارے ہمی ہمیں ہو ہوئی ہوئیں ہوئے ہوا\n",
        " ہوبہو ہوتا ہوتی ہوتیں ہوتے ہونا ہونگے ہونی ہونے ہوں ہی ہیلو ہیں ہے یا یات یعنی یک یہ یہاں یہی یہیں\n",
        "\"\"\".split())\n",
        "\n",
        "\n",
        "def remove_stopwords(text: str):\n",
        "    return \" \".join(word for word in text.split() if word not in STOP_WORDS)\n"
      ],
      "metadata": {
        "id": "qlVvMvbMe3OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urduhack.models.lemmatizer import lemmatizer\n",
        "def lemitizeStr(str):\n",
        "    lemme_str = \"\"\n",
        "    temp = lemmatizer.lemma_lookup(str)\n",
        "    for t in temp:\n",
        "        lemme_str += t[0] + \" \"\n",
        "    \n",
        "    return lemme_str"
      ],
      "metadata": {
        "id": "N04Rh0cMizcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SENTENCE_LENGTH = -9999"
      ],
      "metadata": {
        "id": "6KB0Kd7trCqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import re\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.blank('ur')\n",
        "\n",
        "entries = os.listdir(\"/content/drive/MyDrive/Urdu-Summary-Dataset/urdu-dataset\")\n",
        "files_data = []\n",
        "for entry in entries:\n",
        "    f = open(\"/content/drive/MyDrive/Urdu-Summary-Dataset/urdu-dataset/\" + entry, encoding=\"utf8\")\n",
        "    file_data = {}\n",
        "    text = \"\"\n",
        "\n",
        "    is_summary = False\n",
        "    summary_line_no = 10000000\n",
        "    summary_text = \"\"\n",
        "    for i, line in enumerate(f):\n",
        "        if \"Summary\" in line:\n",
        "            is_summary = True\n",
        "            summary_line_no = i + 1\n",
        "            continue\n",
        "        else:\n",
        "            if i == 1:\n",
        "                file_data[\"Title\"] = line.strip()\n",
        "            elif i >= 4 and is_summary == False:\n",
        "                text += line.strip()\n",
        "\n",
        "        if i >= summary_line_no:\n",
        "            summary_text += line.strip()\n",
        "\n",
        "    file_data[\"Text\"] = text\n",
        "    file_data[\"Summary\"] = summary_text\n",
        "    file_data['FileName'] = entry\n",
        "    category = re.findall('([a-zA-Z ]*)\\d*.*', entry)\n",
        "    file_data['category'] = category[0]\n",
        "    text_sentences = sentence_tokenizer(text)\n",
        "    file_data[\"Text_Sents\"] = text_sentences\n",
        "    summary_sentences = sentence_tokenizer(summary_text)\n",
        "    file_data['Summary_Sents'] = summary_sentences\n",
        "\n",
        "\n",
        "\n",
        "    #Normalize the text (Remove diacritictics)\n",
        "    text_pre_process = normalize(text)\n",
        "    summary_pre_process = normalize(summary_text)\n",
        "\n",
        "    # Remove Stop word\n",
        "    text_pre_process = remove_stopwords(text_pre_process)\n",
        "    summary_pre_process = remove_stopwords(summary_pre_process)\n",
        "    \n",
        "    # Lemmatization\n",
        "    text_pre_process = lemitizeStr(text_pre_process)\n",
        "    summary_pre_process = lemitizeStr(summary_pre_process)\n",
        "\n",
        "    #Generate Text Sents Tokens with Pre Processing.\n",
        "    text_sentences_tokenized = []\n",
        "    for text_sent in text_sentences:\n",
        "      sentence_pre_process = normalize(text_sent)\n",
        "      sentence_pre_process = remove_stopwords(sentence_pre_process)\n",
        "      sentence_pre_process = lemitizeStr(sentence_pre_process)\n",
        "\n",
        "      sen_tokenized = []\n",
        "      sent_tokenized = nlp(sentence_pre_process)\n",
        "      for word in sent_tokenized:\n",
        "          sen_tokenized.append(word.text)\n",
        "\n",
        "      #update the max sentence length\n",
        "      if len(sen_tokenized) > MAX_SENTENCE_LENGTH:\n",
        "       MAX_SENTENCE_LENGTH = len(sen_tokenized)\n",
        "      text_sentences_tokenized.append(sen_tokenized)\n",
        "\n",
        "    \n",
        "    summary_sentences_tokenized = []\n",
        "    for summary_sen in summary_sentences:\n",
        "      summary_sent_pre_process = normalize(summary_sen)\n",
        "      summary_sent_pre_process = remove_stopwords(summary_sent_pre_process)\n",
        "      summary_sent_pre_process = lemitizeStr(summary_sent_pre_process)\n",
        "\n",
        "\n",
        "      summ_sen_tokenized = []\n",
        "      summary_sent_tokenized = nlp(summary_sent_pre_process)\n",
        "      for word in summary_sent_tokenized:\n",
        "        summ_sen_tokenized.append(word.text)\n",
        "\n",
        "      #update the max sentence length\n",
        "      if len(summ_sen_tokenized) > MAX_SENTENCE_LENGTH:\n",
        "        MAX_SENTENCE_LENGTH = len(sen_tokenized)\n",
        "      summary_sentences_tokenized.append(summ_sen_tokenized)\n",
        "\n",
        "\n",
        "    file_data['Text_Sents_Tokenized'] = text_sentences_tokenized\n",
        "    file_data['Summary_Sents_Tokenized'] = summary_sentences_tokenized\n",
        "    files_data.append(file_data)\n",
        "    f.close()\n"
      ],
      "metadata": {
        "id": "HDUpuAelus_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MAX_SENTENCE_LENGTH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhZdx1j2tW6X",
        "outputId": "a99582c2-c481-43cb-a5a9-8512542fd1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])\n",
        "\n",
        "        token_sequences.append(token_sequence)\n",
        "\n",
        "    return token_sequences\n",
        "\n",
        "\n",
        "def to_categorical(sequences, categories):\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)\n",
        "\n",
        "\n",
        "def get_words(sentences):\n",
        "    words = set([])\n",
        "    for sentence in sentences:\n",
        "        for word in sentence:\n",
        "            words.add(word)\n",
        "    return words\n",
        "\n",
        "\n",
        "def get_tags(sentences_tags):\n",
        "    tags = set([])\n",
        "    for tag in sentences_tags:\n",
        "        for t in tag:\n",
        "            tags.add(t)\n",
        "    return tags\n",
        "\n",
        "\n",
        "def get_train_sentences_x(train_sentences, word2index):\n",
        "    train_sentences_x = []\n",
        "    for sentence in train_sentences:\n",
        "        sentence_index = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                sentence_index.append(word2index[word])\n",
        "            except KeyError:\n",
        "                sentence_index.append(word2index['-OOV-'])\n",
        "\n",
        "        train_sentences_x.append(sentence_index)\n",
        "    return train_sentences_x\n",
        "\n",
        "\n",
        "def get_test_sentences_x(test_sentences, word2index):\n",
        "    test_sentences_x = []\n",
        "    for sentence in test_sentences:\n",
        "        sentence_index = []\n",
        "        for word in sentence:\n",
        "            try:\n",
        "                sentence_index.append(word2index[word])\n",
        "            except KeyError:\n",
        "                sentence_index.append(word2index['-OOV-'])\n",
        "        test_sentences_x.append(sentence_index)\n",
        "    return test_sentences_x\n",
        "\n",
        "\n",
        "def get_train_tags_y(train_tags, tag2index):\n",
        "    train_tags_y = []\n",
        "    for tags in train_tags:\n",
        "        train_tags_y.append([tag2index[t] for t in tags])\n",
        "    return train_tags_y\n",
        "\n",
        "\n",
        "def get_test_tags_y(test_tags, tag2index):\n",
        "    test_tags_y = []\n",
        "    for tags in test_tags:\n",
        "      tag_to_index_list = []\n",
        "      for t in tags:\n",
        "        tag_to_index_list.append(tag2index[t])\n",
        "\n",
        "      # test_tags_y.append([tag2index[t] for t in tags])\n",
        "      test_tags_y.append(tag_to_index_list)\n",
        "    return test_tags_y"
      ],
      "metadata": {
        "id": "vj9B7qMgpq4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast\n",
        "\n",
        "tagged_sentences = codecs.open(\"/content/drive/MyDrive/data_lstm.txt\", encoding=\"utf-8\").readlines()\n",
        "print(tagged_sentences[0])\n",
        "print(\"Tagged sentences: \", len(tagged_sentences))\n",
        "sentences, sentence_tags = [], []\n",
        "for tagged_sentence in tagged_sentences:\n",
        "    sentence, tags = zip(*ast.literal_eval(tagged_sentence))\n",
        "    sentences.append(np.array(sentence))\n",
        "    sentence_tags.append(np.array(tags))\n",
        "(train_sentences,\n",
        " test_sentences,\n",
        " train_tags,\n",
        " test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)\n",
        "words = get_words(train_sentences)\n",
        "tags = get_tags(train_tags)"
      ],
      "metadata": {
        "id": "ayW2sYsdSOsk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ebd1a6-d03d-43f6-9181-aa8d719c054d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('’', 'PN'), ('میرے', 'G'), ('بھائی', 'NN'), ('کا', 'P'), ('ای', 'PN'), ('میل', 'U'), ('آیاہے', 'VB'), ('۔', 'SM')]\n",
            "\n",
            "Tagged sentences:  36314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
        "word2index['-PAD-'] = 0\n",
        "word2index['-OOV-'] = 1\n",
        "\n",
        "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
        "tag2index['-PAD-'] = 0\n",
        "print(tag2index['RD'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O559SccKo_Fm",
        "outputId": "39dfa33e-03bd-47e3-eaf0-b21b8904111d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "1BYPHebr8iYd",
        "outputId": "61a5f2e4-1bf7-4347-810d-e8f12e3c95ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences_x = get_train_sentences_x(train_sentences, word2index)\n",
        "test_sentences_x = get_test_sentences_x(test_sentences, word2index)\n",
        "\n",
        "train_tags_y = get_train_tags_y(train_tags, tag2index)\n",
        "test_tags_y = get_test_tags_y(test_tags, tag2index)"
      ],
      "metadata": {
        "id": "V-NSTnTCpYEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "FukRuDjp-Xj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install Keras-Preprocessing \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU6FTPVxyI0N",
        "outputId": "c4c3aad2-e1c4-4ebd-c97d-ba21d596f5c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_LENGTH = len(max(train_sentences_x, key=len))\n",
        "print(MAX_LENGTH)\n",
        "train_sentences_x = pad_sequences(train_sentences_x, maxlen=MAX_LENGTH, padding='post')\n",
        "test_sentences_x = pad_sequences(test_sentences_x, maxlen=MAX_LENGTH, padding='post')\n",
        "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
        "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3UgQSbCper2",
        "outputId": "feccda94-e2dd-46e5-9203-fce1ca449e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation,Embedding,InputLayer\n",
        "from keras import optimizers\n",
        "import tensorflow as tf\n",
        "from keras.layers import Bidirectional, TimeDistributed, LSTM\n"
      ],
      "metadata": {
        "id": "_g1QMfhjMKa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        " \n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy"
      ],
      "metadata": {
        "id": "BCBuKpu6n2iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
        "model.add(Embedding(len(word2index), 128))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
        "model.add(TimeDistributed(Dense(len(tag2index))))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer= tf.keras.optimizers.Adam(0.001),\n",
        "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
        "history = model.fit(train_sentences_x, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=10,\n",
        "                    validation_split=0.2).history\n",
        "model.save(\"/content/drive/MyDrive/models/urdu_pos.h5\")\n",
        "\n",
        "scores = model.evaluate(test_sentences_x, to_categorical(test_tags_y, len(tag2index)))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXjh4_H0plTE",
        "outputId": "07e34e5a-6e63-4f9b-a026-e28b60b0de9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "182/182 [==============================] - 57s 261ms/step - loss: 0.3703 - accuracy: 0.9201 - ignore_accuracy: 0.2498 - val_loss: 0.2390 - val_accuracy: 0.9328 - val_ignore_accuracy: 0.3101\n",
            "Epoch 2/40\n",
            "182/182 [==============================] - 31s 168ms/step - loss: 0.1753 - accuracy: 0.9516 - ignore_accuracy: 0.5069 - val_loss: 0.0998 - val_accuracy: 0.9718 - val_ignore_accuracy: 0.7103\n",
            "Epoch 3/40\n",
            "182/182 [==============================] - 26s 145ms/step - loss: 0.0607 - accuracy: 0.9840 - ignore_accuracy: 0.8365 - val_loss: 0.0388 - val_accuracy: 0.9893 - val_ignore_accuracy: 0.8898\n",
            "Epoch 4/40\n",
            "182/182 [==============================] - 22s 121ms/step - loss: 0.0264 - accuracy: 0.9928 - ignore_accuracy: 0.9268 - val_loss: 0.0258 - val_accuracy: 0.9922 - val_ignore_accuracy: 0.9200\n",
            "Epoch 5/40\n",
            "182/182 [==============================] - 22s 121ms/step - loss: 0.0172 - accuracy: 0.9948 - ignore_accuracy: 0.9473 - val_loss: 0.0224 - val_accuracy: 0.9928 - val_ignore_accuracy: 0.9263\n",
            "Epoch 6/40\n",
            "182/182 [==============================] - 21s 116ms/step - loss: 0.0137 - accuracy: 0.9956 - ignore_accuracy: 0.9548 - val_loss: 0.0214 - val_accuracy: 0.9930 - val_ignore_accuracy: 0.9280\n",
            "Epoch 7/40\n",
            "182/182 [==============================] - 19s 103ms/step - loss: 0.0118 - accuracy: 0.9960 - ignore_accuracy: 0.9592 - val_loss: 0.0210 - val_accuracy: 0.9931 - val_ignore_accuracy: 0.9295\n",
            "Epoch 8/40\n",
            "182/182 [==============================] - 20s 112ms/step - loss: 0.0106 - accuracy: 0.9964 - ignore_accuracy: 0.9628 - val_loss: 0.0209 - val_accuracy: 0.9933 - val_ignore_accuracy: 0.9307\n",
            "Epoch 9/40\n",
            "182/182 [==============================] - 19s 102ms/step - loss: 0.0096 - accuracy: 0.9966 - ignore_accuracy: 0.9658 - val_loss: 0.0212 - val_accuracy: 0.9933 - val_ignore_accuracy: 0.9309\n",
            "Epoch 10/40\n",
            "182/182 [==============================] - 18s 99ms/step - loss: 0.0087 - accuracy: 0.9970 - ignore_accuracy: 0.9691 - val_loss: 0.0217 - val_accuracy: 0.9932 - val_ignore_accuracy: 0.9305\n",
            "Epoch 11/40\n",
            "182/182 [==============================] - 18s 99ms/step - loss: 0.0080 - accuracy: 0.9972 - ignore_accuracy: 0.9716 - val_loss: 0.0224 - val_accuracy: 0.9931 - val_ignore_accuracy: 0.9294\n",
            "Epoch 12/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 0.0073 - accuracy: 0.9974 - ignore_accuracy: 0.9739 - val_loss: 0.0236 - val_accuracy: 0.9931 - val_ignore_accuracy: 0.9292\n",
            "Epoch 13/40\n",
            "182/182 [==============================] - 19s 107ms/step - loss: 0.0067 - accuracy: 0.9977 - ignore_accuracy: 0.9761 - val_loss: 0.0235 - val_accuracy: 0.9932 - val_ignore_accuracy: 0.9300\n",
            "Epoch 14/40\n",
            "182/182 [==============================] - 18s 99ms/step - loss: 0.0061 - accuracy: 0.9979 - ignore_accuracy: 0.9785 - val_loss: 0.0248 - val_accuracy: 0.9931 - val_ignore_accuracy: 0.9292\n",
            "Epoch 15/40\n",
            "182/182 [==============================] - 17s 94ms/step - loss: 0.0056 - accuracy: 0.9981 - ignore_accuracy: 0.9806 - val_loss: 0.0255 - val_accuracy: 0.9931 - val_ignore_accuracy: 0.9290\n",
            "Epoch 16/40\n",
            "182/182 [==============================] - 17s 96ms/step - loss: 0.0051 - accuracy: 0.9983 - ignore_accuracy: 0.9824 - val_loss: 0.0274 - val_accuracy: 0.9929 - val_ignore_accuracy: 0.9271\n",
            "Epoch 17/40\n",
            "182/182 [==============================] - 19s 104ms/step - loss: 0.0046 - accuracy: 0.9985 - ignore_accuracy: 0.9844 - val_loss: 0.0282 - val_accuracy: 0.9929 - val_ignore_accuracy: 0.9275\n",
            "Epoch 18/40\n",
            "182/182 [==============================] - 17s 94ms/step - loss: 0.0041 - accuracy: 0.9987 - ignore_accuracy: 0.9863 - val_loss: 0.0290 - val_accuracy: 0.9929 - val_ignore_accuracy: 0.9271\n",
            "Epoch 19/40\n",
            "182/182 [==============================] - 17s 95ms/step - loss: 0.0036 - accuracy: 0.9988 - ignore_accuracy: 0.9879 - val_loss: 0.0305 - val_accuracy: 0.9928 - val_ignore_accuracy: 0.9260\n",
            "Epoch 20/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 0.0033 - accuracy: 0.9989 - ignore_accuracy: 0.9893 - val_loss: 0.0324 - val_accuracy: 0.9927 - val_ignore_accuracy: 0.9248\n",
            "Epoch 21/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 0.0029 - accuracy: 0.9991 - ignore_accuracy: 0.9907 - val_loss: 0.0334 - val_accuracy: 0.9928 - val_ignore_accuracy: 0.9257\n",
            "Epoch 22/40\n",
            "182/182 [==============================] - 18s 101ms/step - loss: 0.0026 - accuracy: 0.9992 - ignore_accuracy: 0.9920 - val_loss: 0.0347 - val_accuracy: 0.9926 - val_ignore_accuracy: 0.9241\n",
            "Epoch 23/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 0.0022 - accuracy: 0.9993 - ignore_accuracy: 0.9933 - val_loss: 0.0366 - val_accuracy: 0.9925 - val_ignore_accuracy: 0.9235\n",
            "Epoch 24/40\n",
            "182/182 [==============================] - 17s 95ms/step - loss: 0.0019 - accuracy: 0.9994 - ignore_accuracy: 0.9942 - val_loss: 0.0382 - val_accuracy: 0.9925 - val_ignore_accuracy: 0.9230\n",
            "Epoch 25/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 0.0017 - accuracy: 0.9995 - ignore_accuracy: 0.9950 - val_loss: 0.0404 - val_accuracy: 0.9925 - val_ignore_accuracy: 0.9226\n",
            "Epoch 26/40\n",
            "182/182 [==============================] - 18s 99ms/step - loss: 0.0015 - accuracy: 0.9996 - ignore_accuracy: 0.9959 - val_loss: 0.0410 - val_accuracy: 0.9924 - val_ignore_accuracy: 0.9222\n",
            "Epoch 27/40\n",
            "182/182 [==============================] - 17s 94ms/step - loss: 0.0013 - accuracy: 0.9997 - ignore_accuracy: 0.9965 - val_loss: 0.0428 - val_accuracy: 0.9923 - val_ignore_accuracy: 0.9210\n",
            "Epoch 28/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 0.0011 - accuracy: 0.9997 - ignore_accuracy: 0.9972 - val_loss: 0.0439 - val_accuracy: 0.9923 - val_ignore_accuracy: 0.9213\n",
            "Epoch 29/40\n",
            "182/182 [==============================] - 17s 94ms/step - loss: 9.8786e-04 - accuracy: 0.9998 - ignore_accuracy: 0.9975 - val_loss: 0.0454 - val_accuracy: 0.9923 - val_ignore_accuracy: 0.9212\n",
            "Epoch 30/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 8.6186e-04 - accuracy: 0.9998 - ignore_accuracy: 0.9979 - val_loss: 0.0479 - val_accuracy: 0.9923 - val_ignore_accuracy: 0.9205\n",
            "Epoch 31/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 7.6307e-04 - accuracy: 0.9998 - ignore_accuracy: 0.9982 - val_loss: 0.0484 - val_accuracy: 0.9923 - val_ignore_accuracy: 0.9204\n",
            "Epoch 32/40\n",
            "182/182 [==============================] - 19s 102ms/step - loss: 7.1571e-04 - accuracy: 0.9998 - ignore_accuracy: 0.9983 - val_loss: 0.0501 - val_accuracy: 0.9922 - val_ignore_accuracy: 0.9197\n",
            "Epoch 33/40\n",
            "182/182 [==============================] - 18s 99ms/step - loss: 6.5105e-04 - accuracy: 0.9998 - ignore_accuracy: 0.9985 - val_loss: 0.0518 - val_accuracy: 0.9922 - val_ignore_accuracy: 0.9195\n",
            "Epoch 34/40\n",
            "182/182 [==============================] - 18s 97ms/step - loss: 5.2848e-04 - accuracy: 0.9999 - ignore_accuracy: 0.9988 - val_loss: 0.0525 - val_accuracy: 0.9922 - val_ignore_accuracy: 0.9199\n",
            "Epoch 35/40\n",
            "182/182 [==============================] - 18s 99ms/step - loss: 4.7640e-04 - accuracy: 0.9999 - ignore_accuracy: 0.9990 - val_loss: 0.0540 - val_accuracy: 0.9921 - val_ignore_accuracy: 0.9194\n",
            "Epoch 36/40\n",
            "182/182 [==============================] - 18s 98ms/step - loss: 4.6409e-04 - accuracy: 0.9999 - ignore_accuracy: 0.9990 - val_loss: 0.0556 - val_accuracy: 0.9922 - val_ignore_accuracy: 0.9201\n",
            "Epoch 37/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 4.5376e-04 - accuracy: 0.9999 - ignore_accuracy: 0.9990 - val_loss: 0.0566 - val_accuracy: 0.9922 - val_ignore_accuracy: 0.9199\n",
            "Epoch 38/40\n",
            "182/182 [==============================] - 18s 100ms/step - loss: 4.6505e-04 - accuracy: 0.9999 - ignore_accuracy: 0.9989 - val_loss: 0.0570 - val_accuracy: 0.9921 - val_ignore_accuracy: 0.9190\n",
            "Epoch 39/40\n",
            "182/182 [==============================] - 18s 98ms/step - loss: 5.0655e-04 - accuracy: 0.9999 - ignore_accuracy: 0.9987 - val_loss: 0.0580 - val_accuracy: 0.9922 - val_ignore_accuracy: 0.9197\n",
            "Epoch 40/40\n",
            "182/182 [==============================] - 18s 101ms/step - loss: 4.7914e-04 - accuracy: 0.9999 - ignore_accuracy: 0.9988 - val_loss: 0.0584 - val_accuracy: 0.9921 - val_ignore_accuracy: 0.9193\n",
            "227/227 [==============================] - 4s 17ms/step - loss: 0.0573 - accuracy: 0.9921 - ignore_accuracy: 0.9203\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 281, 128)          5324928   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 281, 512)         788480    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 281, 41)          21033     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 281, 41)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,134,441\n",
            "Trainable params: 6,134,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = ['ابتدائی نقصان کے بعد معین علی اور مورگن نے ۔'.split()]"
      ],
      "metadata": {
        "id": "VrZj2NDHJlg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict POS Tags for Text and Summary\n",
        "print(len(files_data))\n",
        "for i in range(len(files_data)):\n",
        "  text_samples = files_data[i][\"Text_Sents_Tokenized\"]\n",
        "  summary_samples = files_data[i][\"Summary_Sents_Tokenized\"]\n",
        "\n",
        "  #Text POS Prediction\n",
        "  text_samples_X = []\n",
        "  for s in text_samples:\n",
        "      s_int = []\n",
        "      for w in s:\n",
        "          try:\n",
        "              word_to_index = word2index[w]\n",
        "              s_int.append(word_to_index)\n",
        "          except KeyError:\n",
        "              s_int.append(word2index['-OOV-'])\n",
        "      text_samples_X.append(s_int)\n",
        "  \n",
        "  text_samples_X = pad_sequences(text_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "  predictions_text = model.predict(text_samples_X)\n",
        "  pred_text_sentences_tags = logits_to_tokens(predictions_text, {i: t for t, i in tag2index.items()})\n",
        "\n",
        "  pred_text_sent_word_tag = []\n",
        "  for index, value in  enumerate(pred_text_sentences_tags):\n",
        "    word_tags = [(word, tag) for word, tag in zip(text_samples[index], value)]\n",
        "    pred_text_sent_word_tag.append(word_tags)\n",
        "\n",
        "  files_data[i][\"Text_Sents_POS\"] = pred_text_sent_word_tag\n",
        "\n",
        "  #Summary POS Prediction:\n",
        "  summary_samples_X = []\n",
        "  for s in summary_samples:\n",
        "      s_int = []\n",
        "      for w in s:\n",
        "          try:\n",
        "              s_int.append(word2index[w])\n",
        "          except KeyError:\n",
        "              s_int.append(word2index['-OOV-'])\n",
        "      summary_samples_X.append(s_int)\n",
        "  \n",
        "  summary_samples_X = pad_sequences(summary_samples_X, maxlen=MAX_LENGTH, padding='post')\n",
        "  print(\"Prediction of Document: \", i)\n",
        "  try:\n",
        "    predictions_summary = model.predict(summary_samples_X)\n",
        "  except:\n",
        "    print(files_data[i][\"FileName\"])\n",
        "    print(summary_samples_X)\n",
        "  pred_summary_sentences_tags = logits_to_tokens(predictions_summary, {i: t for t, i in tag2index.items()})\n",
        "\n",
        "  pred_summary_sent_word_tag = []\n",
        "  for index, value in  enumerate(pred_summary_sentences_tags):\n",
        "    word_tags = [(word, tag) for word, tag in zip(summary_samples[index], value)]\n",
        "    pred_summary_sent_word_tag.append(word_tags)\n",
        "\n",
        "  files_data[i][\"Summary_Sents_POS\"] = pred_summary_sent_word_tag\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZXuTywBLMvy",
        "outputId": "15db7545-c63f-4ffb-d031-d94dceefbe49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "1/1 [==============================] - 1s 649ms/step\n",
            "Prediction of Document:  0\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  1\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  2\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  3\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Prediction of Document:  4\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "Prediction of Document:  5\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Prediction of Document:  6\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "3/3 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  7\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Prediction of Document:  8\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Prediction of Document:  9\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Prediction of Document:  10\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Prediction of Document:  11\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Prediction of Document:  12\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Prediction of Document:  13\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Prediction of Document:  14\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Prediction of Document:  15\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Prediction of Document:  16\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Prediction of Document:  17\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Prediction of Document:  18\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Prediction of Document:  19\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Prediction of Document:  20\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  21\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  22\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  23\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  24\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  25\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  26\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  27\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  28\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  29\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Prediction of Document:  30\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  31\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  32\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  33\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  34\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  35\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  36\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  37\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  38\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  39\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  40\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  41\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Prediction of Document:  42\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  43\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  44\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  45\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "3/3 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  46\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  47\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "Prediction of Document:  48\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  49\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  50\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Prediction of Document:  51\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  52\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  53\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  54\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  55\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  56\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  57\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  58\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  59\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  60\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  61\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  62\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Prediction of Document:  63\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  64\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  65\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  66\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  67\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  68\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  69\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Prediction of Document:  70\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  71\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  72\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  73\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  74\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  75\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Prediction of Document:  76\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  77\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  78\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  79\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Prediction of Document:  80\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Prediction of Document:  81\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Prediction of Document:  82\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Prediction of Document:  83\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "Prediction of Document:  84\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Prediction of Document:  85\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "Prediction of Document:  86\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "Prediction of Document:  87\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Prediction of Document:  88\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Prediction of Document:  89\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Prediction of Document:  90\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Prediction of Document:  91\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Prediction of Document:  92\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "Prediction of Document:  93\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Prediction of Document:  94\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Prediction of Document:  95\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "Prediction of Document:  96\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Prediction of Document:  97\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "Prediction of Document:  98\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "Prediction of Document:  99\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urduhack.utils.pickle_dump(\"/content/drive/MyDrive/corpures_preprocessed\", files_data)"
      ],
      "metadata": {
        "id": "wlss3JH65FuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KYsZK4bV_gr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_list = urduhack.utils.pickle_load(\"/content/drive/MyDrive/corpures_preprocessed\")\n",
        "print(len(load_list))"
      ],
      "metadata": {
        "id": "AHkmmLJy6H4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a1c7a4-282a-42a3-f21b-f44e5f0252e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "# Shuffle dataset\n",
        "shuffle(load_list)\n",
        "\n",
        "# Get 75 % Train set and get extract features for it.\n",
        "len_train_dataset = int(0.75*len(load_list))\n",
        "train_dataset = load_list[0:len_train_dataset]\n",
        "urduhack.utils.pickle_dump(\"/content/drive/MyDrive/corpures_preprocessed_train\", train_dataset)\n",
        "\n",
        "# Pickle dump the 25 % test set.\n",
        "test_dataset = load_list[len_train_dataset: len(load_list)]\n",
        "urduhack.utils.pickle_dump(\"/content/drive/MyDrive/corpures_preprocessed_test\", test_dataset)\n",
        "\n",
        "print(len(train_dataset) + len(test_dataset))"
      ],
      "metadata": {
        "id": "uVI91RTdvami",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01da8e48-4f76-40a9-87d6-17d5c2ebc05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for train_doc in train_dataset:\n",
        "  print(train_doc[\"FileName\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZX4YWzqx1-p",
        "outputId": "58e367f0-1be3-4bd8-af36-8434d5634005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sport11.txt\n",
            "CurrentAfr12.txt\n",
            "Int11.txt\n",
            "Ent07.txt\n",
            "CurrentAfr01.txt\n",
            "Int35.txt\n",
            "Int23.txt\n",
            "CurrentAfr13.txt\n",
            "Int02.txt\n",
            "Ent18.txt\n",
            "Ent01.txt\n",
            "Int17.txt\n",
            "Int22.txt\n",
            "Ent21.txt\n",
            "Sport13.txt\n",
            "CurrentAfr10.txt\n",
            "Sport09.txt\n",
            "Ent14.txt\n",
            "Int15.txt\n",
            "Int06.txt\n",
            "Ent02.txt\n",
            "Ent12.txt\n",
            "Ent11.txt\n",
            "Health05.txt\n",
            "Health02.txt\n",
            "Ent16.txt\n",
            "Int26.txt\n",
            "Int04.txt\n",
            "Ent09.txt\n",
            "CurrentAfr03.txt\n",
            "Sport15.txt\n",
            "Int05.txt\n",
            "CurrentAfr11.txt\n",
            "Ent28.txt\n",
            "Int14.txt\n",
            "Int39.txt\n",
            "Int40.txt\n",
            "Int03.txt\n",
            "CurrentAfr06.txt\n",
            "Sport16.txt\n",
            "Int18.txt\n",
            "Int24.txt\n",
            "Int28.txt\n",
            "Int37.txt\n",
            "Ent24.txt\n",
            "CurrentAfr09.txt\n",
            "Int36.txt\n",
            "Ent08.txt\n",
            "CurrentAfr02.txt\n",
            "Int08.txt\n",
            "Ent22.txt\n",
            "Sport14.txt\n",
            "Ent03.txt\n",
            "Health01.txt\n",
            "Ent19.txt\n",
            "Ent15.txt\n",
            "Int07.txt\n",
            "Ent20.txt\n",
            "Ent23.txt\n",
            "Sport06.txt\n",
            "Ent10.txt\n",
            "Health07.txt\n",
            "Int16.txt\n",
            "CurrentAfr14.txt\n",
            "Ent17.txt\n",
            "CurrentAfr07.txt\n",
            "Ent27.txt\n",
            "Int10.txt\n",
            "Int20.txt\n",
            "Int34.txt\n",
            "Int25.txt\n",
            "Ent06.txt\n",
            "Int12.txt\n",
            "Health04.txt\n",
            "Int27.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_samples_X)\n",
        "print(predictions, predictions.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "x4VuPxceMCox",
        "outputId": "d2758172-b793-411e-ec25-2eadd8b72d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5db4fa7c7a0b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_samples_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 281), found shape=(None, 270)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sentences_tags = logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
        "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSXY0ki8mmjp",
        "outputId": "e8907666-41e9-48d0-ce69-24aa44308289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['ADJ', 'NN', 'P', 'NN', 'PN', 'PN', 'CC', 'PN', 'P', 'SM', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-', '-PAD-']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_sent_word_tag = []\n",
        "#for index, value in enumerate(chars):\n",
        "for index, value in  enumerate(pred_sentences_tags):\n",
        "  word_tags = [(word, tag) for word, tag in zip(test_samples[index], value)]\n",
        "  pred_sent_word_tag.append(word_tags)\n",
        "\n",
        "print(pred_sent_word_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbn32rIleJzf",
        "outputId": "d937476a-fff9-44dd-cb89-2eff3cc48dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('ابتدائی', 'ADJ'), ('نقصان', 'NN'), ('کے', 'P'), ('بعد', 'NN'), ('معین', 'PN'), ('علی', 'PN'), ('اور', 'CC'), ('مورگن', 'PN'), ('نے', 'P'), ('۔', 'SM')]]\n"
          ]
        }
      ]
    }
  ]
}